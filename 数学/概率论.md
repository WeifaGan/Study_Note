## 第一章 事件的概率

## 1.1 概率是什么

* 概率用来衡量某件事情发生的机率大小

### 1.1.1 主观概率

### 1.1.2 试验与事件

* 概率并不是指已经发生了事情，而是某件事，它可能发生或者不发生，发生与否，要到有关试验有了结果才知道

* 概率论中的事件是这样子定义的：
  * 有一个明确的试验
  * 这个试验的全部结果是试验前就明确的。有些情况不能确切知道试验的全部结果，但是又范围也可以的。
  * 又一个明确的陈述

### 1.1.3 古典概率

* 古典概率是计算等可能性的事件
* 古典概率的计算主要是基于排列组合

* 局限
  * 全部试验结果是个数有限且可能性的。
  * 但是这些情况的计算需要用到几何概率

### 1.1.4 概率的统计定义

* 概率统计定义要旨是说那某个事件出现的频率作为该事件的估计。
* 这个概念的直观背景：一个事件出现的可能性大小，应该在多次重复试验中出现的频率程度去刻画。当时这又不足之处，就是频率只是概率的估计并非概率本身，但是只要试验次数无限次，频率就是概率的极限

* 概率的统计定义的重要性
  * 永远不可能知道事件的概率，只是提供了估计概率的方法
  * 提供了检验理论正确与否的准测。根据某个理论算出的某件事件的概率，这个理论是否与实际相符，这类问题属于假设检验

## 1.3 事件的运算、条件概率和独立性

 ### 1.3.1 事件蕴含、包含及相等

###  1.3.2 事件互斥和对立

* 互斥：不可能同时发生，但是可以都不发生
* 对立：补事件

### 1.3.3 事件和

### 1.3.4 事件的加法定理

* 互斥事件之和的概率=各事件概率之和

### 1.3.5 事件的积(交)、事件的差

* C={A,B都发生}，事件C称为A、B的积
* 事件差A-B={A发生，B不发生}，A-B=AB(非)

### 1.3.6 条件概率

* 在某件事情已发生的的情况下发生另外一件事件

* $P(A|B)=P(AB)/P(B)$

### 1.3.7 事件独立性，概率乘法定理

* 满足$P(AB)=P(A)P(B)$,事件独立
* 独立事件的任一部分独立
* 某一系列事件独立，则其任一部分改为对立事件，所得事件仍然独立

### 1.3.8全概率公式和贝叶斯公式

**全概率公式**

* “全”部概率P(A)被分解成许多部分之和：$P(A)=P(B_1)P(A|B_1)+P(B_2)P(A|B_2)...$
* 理论和使用意义
  * 简化事件
  * 把$B_i$看成导致事件A发生一种可能途径

**贝叶斯公式**：

* $P(B_i|A)=P(AB_i)/P(A)$=条件概率/全概率
* 哲理意义



## 第二章 随机变量及概率分布

## 2.1 一维随机变量

### 2.1.2 离散型随机变量的分布

* X的分布函数

* 任何随机变量x,其分布函数F具有下面的一般性质：
  * F是单调非降的，当x1<x2时候，有F1<F2
  * 当x取向无穷时候，F趋向1，当x取向负无穷，F趋向0
* 二项分布两个重要条件：
  * 每次实验的条件是稳定的
  * 每次实验是独立的
* 泊松分布
  * 二项分布的极限得到
  * 较难计算的二项分布可以转化为泊松分布

### 2.1.3 连续型随机变量的分布

* 密度函数解释：无穷小区段内单位长的概率，反应了某点的概率的”密集程序“
* 三条性质
* 正态分布
* 指数分布
* 均匀分布

## 2.2 多维随机变量(随机向量)

### 2.2.1 离散型随机向量分布

* 一个随机向量，每个分量都是一维离散型随机变量，那么随机向量是离散性的。
* 随即向量的概率函数(概率分布)

### 2.2.2 连续型随机向量分布

* 随机向量的全部取值不满整个区域

* 概率密度函数
* 二维正态分布

### 2.2.3边缘分布

* n维随机变量每个分量各自的分布，成为边缘分布
* 边缘分布求法
  * 离散
  * 连续

## 2.3 条件概率分布与随机变量的独立性

### 2.3.2 离散型随机变量的条件概率分布

* 计算公式：根据条件概率和边缘分布
  * $P(x_2|x_1)=p_{ij}/{\sum{p_{ik}}}$

### 2.3.3 连续型随机变量的条件分布

* 也是又条件概率和边缘分布得出
* 条件分布函数

## 2.4 随机变量的独立性

* 一个变量的分布和两外一个变量取什么值没有关系，称这两个变量是独立的或者变量的无条件概率等于另外一变量条件下的概率
* 定义
  * $f(x_1..x_n)=f_1(x_1)..f_2(x_n)$
  * $P(X_1=a_1...X_n=a_n)=P(X_1=a_1)...P(X_n=a_n)$
* 定理
  * 变量取值跟其他变量无关
  * 连续性随机变量表示为n个函数之积
  * 部分独立