# Objects as Points

## 摘要

目标检测将图像的目标识别为轴对称的boxes。很多成功的目标检测器会列出几乎详尽的潜在对象位置，并对每个位置分类。这是很浪费时间，没有效率和需要额外的后处理。在文中，作者采用不同的方法，将目标建模为一个点-边框中心点。作者的检测器使用关键点估计寻找中心点和回归其他对象属性。例如大小，3D位置，方向甚至是姿态。相比与bounding box的检测器，CenterNet是端到端的可微的，简单的，更快的，和更准确的。在MSCOCO数据集，CenterNet是达到了最好的速度-精度平衡，142FPS达到28.1%AP，52FPS达到37.4%AP，多尺度训练1.4FPS达到45.1%AP。

## 1. Introduction

单阶段检测器在图像上滑动可能的bounding boxes的复杂排列排列，然后没有指定边框的内容直接分类。两个阶段的检测器对每个潜在边框重复计算图像特征，然后分类这些特征。然后。后处理(NMS)通过计算边框IOU来删除同一个实例的重复检测。这种后处理很难微分和训练，所以现在大多数检测器不是端到端可训练的。尽管如此，在过去五年，这种思想取得了较大的成功。然而，基于滑窗的目标检测器有一些浪费时间，因为他们需要列举所有可能的目标位置和大小。

文中作者提供了一个更简单更高效选择。作者将一个对象表示为在它边框中心的一个点，其他性能，例如对象大小，维度，3D扩展，方向和姿态可以**从中心位置的图像特征直接回归**。然后目标检测是一种标准的关键点检测问题。作者简单地把输入图像喂入全卷基层然后产生一个heatmap。在这张heatmap中的峰值对应对象的中心。在每个峰值的图像特征预测对象边框的高和宽。模型使用标准的监督学习训练。推理是单个网络的前向传播，没有nms的后处理。

作者的方法是通用的，可以经过微小的改动就可以扩展到其他任务。作者提供的实验通过在每个中心点预测额外的输出进行3D对象检测和多人姿态估计。对与3D边框估计，作者对对象的绝度深度、3D边框估计维度和对象方向进行回归。对于人的姿态估计，作者把2D关节点的位置看成是中心点的偏移并从中心点的位置直接回归它们。



## 2.Related Work

**区域分类的目标检测**：RCNN从大量候选区域中列举目标位置，剪裁然后送入网络进行分类。而Fast-RCNN则对图像特征进行剪裁，节省计算消耗。但是这两种方法依赖慢的低级的候选区域方法。

**隐式anchors的目标检测**：Faster RCNN 在检测网络中产生候选区域。它在低分辨率图像网格中采样固定大小的边框(anchors)并把分类成是否为前景。Anchor与GT对象重叠率大于0.7记为前景，小于0.3记为背景，其他情况忽略。每一个产生的候选区再分类。候选区分类器改成多分类是单阶段检测器的基础。单阶段检测器的改进包括了anchor形状先验，不同特征分辨率，不同样本间loss的重复加权。

作者的方法类似于基于anchor的单阶段方法。一个中心点可以认为是一个不可形状的anchor，但是有一些重要的不同点。第一，CenterNet的anchor仅仅基于位置，不是边框重叠比例。前景背景分类没有手动阈值。第二，每个对象仅仅有一个正的"anchor"，所以不需要NMS。仅仅简单提取关键点heatmap的局部峰值。第三，相对传统的目标检测器，CenterNet使用了更大的输出分辨率。这消除了多anchor的需求。

**关键点估计的目标检测**:CornerNet把两个边框角作为关键点来检测。ExremeNet检测所有对象中心点，和most top，left，bottom，right。这些方法都像CenterNet一样建立在鲁棒的关键点估计网络。可是它们在关键点检测之后需要组合分组阶段，这样子降低了算法速度。CenterNet简单的提取每个对象的中心点，不需要组合或者后处理。

**单目3D目标检测**:3D边框估计驱动了自动驾驶，使用slow-RCNN风格框架的Deep3Dbox，首先检测2d对象然后把每个对象喂入3D估计网络。3D RCNN 在Faster-RCNN上增加了一个附加头部，然后是3D投影。 Deep Manta使用在许多任务上受过训练的从粗到细的Faster-RCNN 。 我们的方法类似于Deep3Dbox或3DRCNN的单阶段版本。 因此，CenterNet比竞争方法简单得多，而且速度更快。

## 3. Preliminary

输入图像$I \in R^{W\times H \times 3}$，目标是产生一个关键点heatmap$\hat{Y}\in[0,1]^{\frac{W}{R}\times \frac{H}{R} \times C}$，$R$为输入步长，$C$是关键点类型的数量。姿态估计中$C=17$，目标检测中$C=80$。$\hat{Y}_{x,y,c}=1$对应检测的关键点，$\hat{Y}_{x,y,c}=0$则为背景。作者使用几个不同的全卷积编码解码网络从$I$来预测$\hat{Y}$:**hourglass,ResNet,DLA。**

对于每个类别$c$的GT关键点$p$，计算它的低分辨率$\overline{p}=\frac{p}{R}$的向下取整。作者通过使用高斯核把所有gt关键点映射到heatmap $Y\in[0,1]^{\frac{W}{R}\times \frac{W}{R} \times C}$，如果同一个类的高斯重叠，则选择像素最大那个。Focal Loss:

![](https://gitee.com/weifagan/MyPic/raw/master/img/CenterNet loss.png)

为了弥补由输出步长导致的离散化误差，额外预测局部偏移。所有类别$c$共享一个偏移预测。

![](https://gitee.com/weifagan/MyPic/raw/master/img/CenterNet loss off.png)

## 4. Objects as Points

$(x^{(k)}_1,y^{(k)}_1,x^{(k)}_2,y^{(k)}_2)$为类别$c_k$的对象$k$的边框，它的中心点位于$(\frac{x^{(k)}_1+y^{(k)}_1}{2},(\frac{x^{(k)}_1+y^{(k)}_1}{2})$。作者用关键点估计器$\hat{Y}$来预测所有中心点。此外，还回归每个对象$k$的大小$s_k$。为了限制计算负担，对所有目标仅仅使用一个单尺寸预测$\hat{S}$。

![](https://gitee.com/weifagan/MyPic/raw/master/img/CenterNet loss size.png)

总的目标函数：

![](https://gitee.com/weifagan/MyPic/raw/master/img/CenterNet loss det.png)

对于每个位置，网络预测一共$C+4$输出。所有输出共享一个共同的全卷积backbone网络。对于每个模态，主干的特征通过3×3卷积、ReLU卷积和另一个1×1卷积

**From points to bounding boxes**:推理时候，首先单独地给每个类别提取heatmap的峰值，作者检测所有大于等于它周围8个邻域的值响应并保持top100个峰值。作者使用关键点的值$\hat{Y}_{x_iy_ic}$作为它的检测自信度并产生边框：

![](https://gitee.com/weifagan/MyPic/raw/master/img/CenterNet box.png)

所有输出直接从关键点估计产生，不需要NMS或者其他后处理。峰值关键点提取看作是充足NMS替代方案并可以使用一个3x3的maxpool操作有效实现。

### 4.1 3D检测

先略了。略略略。。。

### 4.2 人体姿态估计

先略了。略略略

## 5. 执行细节

对四个结构进行实验：ResNet-8，ResNet-101，DLA-34和Hourglass-104。作者使用dcn层来修改ResNets和DLA-34.

剩下的先略了。

### 6.1 实验

结果见论文



# 个人总结

**1）用自己的话总结文章思路**:

CenterNet把对象看做一个中心点。首先在全卷积网络上产生一个heatmap，然后heatmap上会有预测出来的中心点(峰值)，其他性能如大小维度等也可以从中心位置的图像特征进行回归。当然原图GT的坐标也要通过步长和高斯核映射到heatmap，这样子才能产生loss。

**2）关键因素**:

* 把对象建模成一个中心点并在目标检测转换为关键点估计。

**3）为我所用**

* 把对象建模成一个中心点并在目标检测转换为关键点估计

* centernet网络

  