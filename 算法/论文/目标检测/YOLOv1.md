# YOLO V1

# YOLO V1的总体介绍

RCNN开创性地提出了候选区的方法，它先是在图片上搜索出可能存在对象的候选区，然后在候选区再识别对象。后来再发展出fast RCNN和faster RCNN,不过两个阶段的RCNN系列的检测方法速度很慢，因为他们都是两个阶段的处理模式：提出候选区再识别对象。

RCNN虽说是搜索出候选区，但是识别出对象后还是要对边框进行微调的。既然如此，又何必辛辛苦苦地去找候选区，大概画个范围不就行了吗？对！YOLO就是这么干的！YOLO V1把图片划分为7x7=49个网格，每个网格预测2个边框，共98个预测边框。它创造性地把RCNN两个阶段的处理合二为一，它把目标检测看成一个回归问题，直接预测物体的边框和识别对象。所以YOLO V1的速度非常快，能够实时达到45fps，网络较小的fast YOLO V1的版本在保持其他实时检测系统的两倍mAP的同时，速度能实时达到155fps。相比与其他 state-of-the-art 检测系统，YOLO V1虽然有较多的定位误差，但是把背景预测出物体的可能性更低。

# 网络设计

YOLO V1的网络设计借鉴了GoogLeNet，但是与之不同的是，YOLO V1的没有使用inception modules，而是使用1x1卷基层(为了整合通道信息)+3x3卷基层。YOLO V1模型共24层网络+2层全连接层。模型如下图所示。

![图1 ](https://gitee.com/weifagan/MyPic/raw/master/img/YOLO1_net.png)

## 输入

从图中可以看到，网络输入的大小为448x448，输入图片会被分成7x7个网格，每个网格只负责预测一个对象，对象的中心位置落在哪个网格，哪个网格就负责对它进行预测，譬如下图中的狗的中心落在红色的网格，那么该网格的两个预测边框就负责预测那只狗。

![图1 ](https://gitee.com/weifagan/MyPic/raw/master/img/YOLO1_grid.png)

## 输出

检测网络的输出为7x7x30的张量(SxS+(Bx5+C)，SxS表示输入图片被划分的网格数量，B表示每个网络预测的边框数量，C表示类别数量，PASCAL VOC 有20 类别，所以C = 20)，输出的7x7是对应于输入图片的7x7个网格的，也就是说输入图片的每个格子都对应一个30维的向量，例如左上角的网格对应于输出张量中左上角的向量。但是这并不意味着7x7x30的张量分别是7x7个网格卷积池化后的结果，事实上，输出的张量是整张图片卷积池化后的得到的。

那么在30维度的向量中包含的信息如下图所示。

![图1 ](https://gitee.com/weifagan/MyPic/raw/master/img/yolov1_30infor.JPG)

**20个对象概率。** 这个对象概率是指条件概率，即在有对象的情况下该对象为某一类的概率，记为$P(c_1|object)，..., P(c_{20}|object)$。

**2个边框位置。** 一个边框位置包括了四个数值，分别是边框中心x坐标，y坐标，边框高度，边框宽度。

**2个边框置信度。** 置信度衡量的是否包含对象且位置准确的程度，置信度高表示这里有个对象并且位置较为准确，置信度低则表示没有对象或者即使有对象，但预测的边框与ground truth偏差较大。置信度用公式表示为$confidence = P(object)*IOU$，其中$P(object)$表示的是有对象的概率，注意区别条件概率$P(c|object)$。而IOU反映了预测边框和ground truth的接近程度。

# 训练

## 标签

网络输出是7x7x30的张量，那么训练时候如何填写30维度的向量标签？在30维度的向量中，包括了20个类别信息，2个边框位置(4个坐标+2个边框的对应的confidence)，如下图所示。

![图1 ](https://gitee.com/weifagan/MyPic/raw/master/img/yolov1_label.JPG)

**20个类别概率。** 譬如说对于物体A，我们根据ground truth算出其中心，若其中心落在网格a，网格a就负责预测物体A，所以网格a对应标签中的A的类别概率是1，其他均是0，因为一个网格只预测一个对象。除了网格a，其他的网格标签中，A的类别概率均为0。这就是所谓的“中心点所在的网格对预测该对象负责”。

**2个边框位置。** 一个网格会预测两个边框，但是真实的边框只有一个，那应该填写边框1还是边框2呢？事实上，那根据预测边框与ground truth的IOU来选择，哪个IOU大就填哪个边框。

**2个confidence。** 置信度的公式： $confidence=P(object)*IOU$。一个网格预测的边框，哪个IOU大，哪个边框就负责预测对象，该边框的$P(object)=1$，$confidence=IOU$，同时ground truth的边框信息也填入该边框中。而另外一个边框的P(object)=0。举个例子，狗的中心位置落在网格a，若ground truth的边框信息填入边框1，那么该边框对应的confidece为1，另外一个边框的则不填入位置信息且confidence为0，该网格的标签如下图所示。

![图1 ](https://gitee.com/weifagan/MyPic/raw/master/img/yolov1_30infor1.JPG)

## 损失函数

在训练的过程中，yolo v1优化以下损失函数：

![图1 ](https://gitee.com/weifagan/MyPic/raw/master/img/yolov1_loss1.JPG)

**符号解释**

* $ 1^{obj}_{i} $ 表示网格$i$ 中有对象
* $1^{obj}_{ij}$ 表示网格$i$ 的第$j$ 个预测框中有对象
* $1^{noobj}_{ij}$ 表示网格$i$的第$j$ 个预测框没有对象

**边框误差**   

* $1^{obj}_{ij} $ 表示只负责预测对象的边框(与ground truth有较大的IOU)才算入误差

* 在计算边框宽度高度误差时候，先对高度和宽度取平方根，目的是为了降低大边框和小边框的敏感度问题。因为大边框对差值的敏感度较低，小边框对差值的敏感度较高。举个下图的例子，假设外面的蓝色大边框是ground truth，里面黄色的是预测的边框，理想性的假设宽度完美匹配，如果单纯地用实际边框高度-预测边框高度，可以看到他们边框误差都是1，但是很明显大边框匹配程度更高。这种实际边框高度-预测边框高度的误差计算方式明显地不合适，所以先取平方根降低敏感度。

  ![图1 ](https://gitee.com/weifagan/MyPic/raw/master/img/yolov1_sensitive.JPG)

* $\lambda_{coord}$ 用于调节边框误差权重(相对于分类误差和置信度误差)，$\lambda_{coord}=5$ 调高位置误差权重(位置位置8个维度，分类误差20个维度，所以该权重取高一点)

**置信度误差**

* $1^{obj}_{ij} $ 表示只负责预测对象的边框(与ground truth有较大的IOU)才算入误差。
* $1^{noobj}_{ij} $ 表示两个没有对象的边框的置信度都算入误差。
* 置信度误差其实有对象和没有对象的边框都算上了。
* $\lambda_{noobj}=0.5$ 以调节权重，降低不存在对象的预测边框的置信度(其实没有对象的网格应该还是是占大多数，这些网格的置信度分数push至0，通常压倒了包含目标的单元格的梯度，导致模型不稳定，从而导致训练早期发散)

**分类误差** 

* 把所有网格的20个分类概率误差平方相加

## 训练方法

* 在ImageNet中进行预训练网络的前20个卷积层，此时输入大小为224x224，目标检测时为了精细的视觉信息，输入大小改为为448x448。
* 在预训练的20个卷积层后加上随机初始化的4个卷积层和2个全连接层进行目标检测任务的训练。
* 把边框的宽和高正则化至[0,1]之间，中心位置x,y使用相对于网格的偏移，也归一化到[0,1]之间。
* 模型在最后一层使用线性激活函数(除了0-1之间的类别概率以外还有其他范围的w,h,x,y的坐标预测)，在其它层使用leaky Relu。
* 学习率第一个epoch从$10^{-3}$慢慢调到$10^{-2}$，继续训练$10^{-2}$ 74个epoch，$10^{-3}$ 30个epoch，$10^{-4}$ 30个epoch。
* dropout：0.5
* 数据增广：随机缩放+随机旋转+随机曝光+在HSV空间增加饱和度

# 测试

**类别预测：** 网络输出的类别概率是一个条件概率$Pr(C_{i}|object)$，即对象存在的条件下为某个类别的概率，一般来说，类别预测直接把这个条件概率作为最终的分类概率就可以了，但是，作者在预测阶段还把该条件概率乘以confidence作为最终输出的类别概率，即$score_{ij}=Pr(C_{ij}|object)*Pr(object)*IOU=Pr(C_{ij}|object)*confidence_{j}$。在预测阶段，也可能存在这种情况，网格不存在物体，即$confidence=0$，但是输出的类别概率$Pr(C_i|object)=0.9$，这就有点不太合理了，所以乘上了$confidence$使其$score_{ij}$为0。

**NMS的流程:**

* NMS的目的在于解决多个边框检测到同一个对象的问题。

* 1.类别预测的score设置阈值，低于这个阈值的边框排除掉(把score设置为0)
* 2.遍历每一个对象类别
* 2.1 遍历每个对象类别的98个得分
* 2.1.1 找到score最大的的对象及其对应的边框big_bbx，添加至输出列表
* 2.1.2 计算score不为0的候选对象，计算其与big_bbx的IOU
* 2.1.3 根据预先设定的IOU阈值，如果高于所算的IOU高于该阈值(重叠程度高)，则把该候选对象排除掉(设置score设置为0)
* 2.1.4 如果所有的边框要么在输出列表，要么score为0，则该对象类别的NMS完成。返回步骤2处理下一个对象。
* 3 输出列表即为预测对象。

# 讨论 

**为什么一个网格要有两个预测框？** 

既然一个网格只能预测一个目标，那为什么要两个预测边框呢？训练时候谁的预测边框与ground truth的IOU更大，谁就负责预测目标，另一个则不预测，其实就是选择预测能力最好的边框。这样设置的一个结果将会使一个单元格对应的边界框更加专业化，其可以分别适用不同大小，不同高宽比的目标，从而提升模型性能，个人理解其实就是为了找多点人干活，然后挑干得最好的呗。

**如果两个对象的物体都落在同一个网格，那怎么办？**

因为一个网格只能预测一个对象，所以这种情况就凉凉了。