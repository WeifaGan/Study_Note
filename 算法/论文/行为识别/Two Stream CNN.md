# Two_stream_CNN

## 1 摘要

视频行为识别的难点在于难以捕获静止帧和帧之间运动的互补信息。作者贡献有三：第一，提出了融合时间和空间的双流卷积网络。第二，虽然训练数据不多，但证明了在多帧密集光流上训练卷积网络可以达到很好的效果。第三，表明了使用两个不同的行为分类数据集进行多任务学习可以增加训练数据和提升表现。



## 2 视频识别双流结构

我们可以把视频分解为时间和空间两种成分。以独立帧形式存在的空间成分蕴含了场景和物体信息。以帧间运行形式存在的时间成分传递了观察者和物体的移动信息。所以，作者很自然地把视频识别模型的结构分成两个流(部分），如下图所示。每个流都是用卷积网络执行，然后融合softmax分数。对于融合方法，作者考虑两种：第一是平均融合，第二是使用堆叠$l_2$归一化的softmax分数作为特征来训练多类线性SVM。

![](https://gitee.com/weifagan/MyPic/raw/master/img/two_stream.PNG)

空间流卷积在单独的视频帧上进行。因为一些行为跟某些特定的物体有着很强的联系，所以静止图像本身也是一个有用的线索。

## 3 光流卷积网络

光流卷积网络的输入是通过几个连续帧的堆叠光流位移场而形成的。由于网络不需要隐式估计运动，因此这输入明确描述了视频帧之间的运动，使得识别更加easy。作者考虑几种光流输入：

![](https://gitee.com/weifagan/MyPic/raw/master/img/optical_flow.PNG)

图中(a)(b)是一对连续视频帧，（c）是密集光流特写(d)位移矢量场的垂直成分$d^x$（e）水平成分$d^y$.

### 3.1 卷积输入配置

**光流堆叠.** 稠密光流可以认为是一些列连续帧$t$到$t+1$之间的位移矢量场$d_t$。$d_t(u,v)$表示为第$t$帧的点$(u,v)$的位移矢量，这位移矢量向着下一帧$t+1$相对应的点移动。矢量场垂直和水平成分$d_t^x$和$d_t^y$可以认为是图像的通道。任意帧$\tau$卷积输入表示为：

![](https://gitee.com/weifagan/MyPic/raw/master/img/two_stream1.PNG)

**轨迹堆叠.**另外一种可替代的运动表示，在几帧之间相同位置采样的光流代替为在运动轨迹采样的的流。$\tau$帧对应的输入表示为：

![](https://gitee.com/weifagan/MyPic/raw/master/img/two_stream2.PNG)

光流堆叠和轨迹堆叠如下图所示

![](https://gitee.com/weifagan/MyPic/raw/master/img/two_stream3.PNG)

**双向光流.** 通过计算在相反方向的额外一系列的位移矢量场来得到双向光流。通过堆叠$\tau$到$\tau+L/2$帧之间的$L/2$的正向光流和堆叠$\tau-L/2$到$\tau$帧之间的$L/2$的反向光流得到输入$I_{\tau}$

**平均流相减.** 一般来说，对网络的输入进行零中心化是有益的，因为他允许模型更好地利用整流非线性。作者考虑使用更加简单的方法：每一个位移矢量场减去它中心向量.

**结构.** 从$I_\tau$中采样一个224x224x2L的子卷作为网络的输入。



## 4 多任务学习

时间卷积流可以使用预训练模型，但是空间卷积流就没办法使用预训练模型了，所以考虑用两个数据集对空间卷积流进行训练，但是这样子可能要手动地去帅选类，因为有些类别可能有重复。

一个可行的方法是进行多任务学习，ConvNet结构最后的全连接层使用两个softmax层，分别学习两个不同的数据集，每个softmax层有自己的loss，最后训练的loss是两个任务loss之和。

## 5 执行细节

**训练.** 使用mini_batch SGD with momentum(0.9)；每次迭代从256个训练视频(类别均衡)中构造256个样本，每个视频中随机取帧；训练空间流网络时候，从选择的帧中剪裁成224x224的子图，然后随机水平翻转和RGB抖动；训练时间流网络，从选择的训练帧中计算光流卷，从光流卷中剪成224x224x2L和翻转；初始学习率为$10^{-2}$，50K训练后学习率为$10^{-3}$，70k之后为$10^{-4}$，80k之后停止训练；fine-tuning时候，14k之后学习率改为$10^{-3}$，20k之后停止。

**测试.** 测试时候，一个视频均衡采样25帧， 通过剪裁和翻转四个角和中心，每个视频得到10个ConNet输入。整个视频的的分类分数通过平均采样帧和其中的剪裁得到。

**在ImageNet ILSVRC-2012上预训练.** 预训练空间流网络时候，对训练集和测试集使用相同的数据扩增(剪裁，翻转，RGB扰动)。验证集上 13.5%的错误率。

**光流** 使用Opencv 工具箱计算得到。为了避免浮点型位移场，把流的水平和垂直成分线性地rescaled到[0,255]，然后使用JPEG压缩(压缩后，流rescaled back 原来地范围)，UCF-101数据集的流 27GB.



## 6 个人执行细节

**参数配置：**L=5,w=225,h= 224,如果一个视频有10帧，那么第1-5帧为一个光流卷，...，第5-9帧为一个光流卷，但是6-10帧无法成为一个光流卷，因为第10帧是最后一帧，无法跟下一帧构建光流，这样子的话，一个n帧的视频，时间流网络只能有n-L个光流卷输入，那空间流网络也只能是n-L个单帧输入了？

